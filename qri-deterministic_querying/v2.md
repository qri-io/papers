<h1 align="center">Deterministic Querying for the Distributed Web</h1>

<p align="center">
<a href="http://www.brendan.nyc">Brendan O'Brien</a><br>
<a href="http://www.cds.caltech.edu/~mhucka/">Michael Hucka</a>
</p>
<p align="center">
July 2017
</p>
<p align="center">
<b>Abstract</b>
</p>

The current infrastructure that underlies the distribution and processing of data over the Internet has enabled dramatic advances in a great variety of human endeavors.  Recent trends promoting open data promise even greater benefits as the number and variety of networked data sets grows.  However, while current technologies and architectures obviously do work, they also show signs of systemic inefficiencies that impede effective large-scale discovery, distribution, preservation and reuse of data sets.  An important source of inefficiencies is rooted in the database-centered pipeline that underlies much of data processing on the Internet today.  The ongoing transition to a content-addressed permanent web offers an opportunity to replace certain software architecture patterns with alternatives that are more efficient and scale more effectively.  We introduce one such new pattern, _deterministic querying_, that combines formalized query definitions, formalized resource descriptions, linked metadata, hashing, and a content-addressed file system.  The result supports database-free reuse of data and data processing results---and can do so on the scale of the global Internet.


# Introduction

Open data is a boon to research and progress [@murray2008open; @piwowar2013data; @lowndes2017our; @gewin2016data].  The continuing growth of data being made available on the Internet promises a windfall of benefits, enabling new discoveries, powering new innovations, and increasing transparency.  This growing open data movement has in part been inspired by and driven by the open _source_ movement, and in fact many of the principles espoused by open data advocates mirror those of open source efforts.  However, open source efforts currently have at least one advantage over open data: the former has effective means of _accumulating results in a common pool_ so that they can be reused rather than reinvented.

Reuse is at the heart of open source.  Developers are encouraged to share their work, and to seek out existing solutions to problems.  As a result, open software builds upon other software, often by composing larger works out of smaller, reusable elements.  This is possible because the reusable software elements---objects, functions, programs, modules---can be reused as static entities incorporated into other works.  The software elements represent intellectual labor: they are answers to questions.  Open-source developers solve new problems in large part by seeking out answers to old questions and then using those answers to build their solutions.  This approach works well in software in part because developers have created precise mechanisms to describe the questions, the forms of the answers, and how to store and exchange them.

By contrast, open data has no comparable, framework-agnostic means of composing solutions by building on old results.  In a practical sense, users of data often reinvent the same solutions: individual users and processes employ computing frameworks to consume datasets, perform operations on the data, and then interpret the results---and then often throw away the data produced by the operations.  This happens because the processed data are either assumed to be of no value to anyone else, or else the cost of storing, managing and sharing intermediate results is assumed to be higher than simply recomputing them on demand.  The consequence is that answers to questions are not being shared, discovered or reused by other potential users of open data.

## Databases and modern software stacks

Central to the problem of treating open data as reusable components is that there is no commonly-accepted methods for using the _output_ of an open data process as the _input_ to another open data process when the processes are not part of a common pipeline.  Methods for reusing computational results in distributed workflows do exist, but they are specific to particular execution environments or frameworks [e.g., @elghandour2012restore].  No common scheme exists for persisting and reusing results of computations produced by widely-used software stacks such as those within today's web applications---the applications that power so many cloud-based services.

The architectural patterns underlying modern software stacks are part of the problem.  One of the greatest barriers to efficient reuse of results is the _positioning of the database_.  In many cases, the database is placed at the heart of a modern web-based application.  Data is stored in the database in a raw form, but then must be encoded into whatever form is consumed by the user at access time.  This approach of _interpreting the data outward_ towards the user makes perfect sense when the primary "view" or representation of this data is in the form of structured HTML:

```
  Database -> HTML Render -> Network -> Web Browser
```

From there, the natural next step is to provide programmatic access to this data via an HTTP API, which is essentially another "view" on the same central database:

```
  Database -> API Encode -> Network -> API Decode
```

This approach is flexible, and there is no question that it works---after all, it serves as the basis of many software systems.  However, this approach is the product of a long history of accumulated technical contexts, some of which are now the source of a great deal of inefficency.  Consider what happens when the results of a data service are processed by another service:

```
  Database -> API Encode -> Network -> API Decode -> Process -> Database
```

In today's software environments, this pattern uses multiple servers to take data out of one database, serve it across a network, decode it, process it, and put it into another database (which in fact may require additional encoding, if the output of the processing does not match how it is stored in the database---and it often does not).  Every step requires separate engineering and long-term software maintenance.  This pattern heavily favors holding the data as closed information because that "opening" this data is an active effort that would require additional engineering time, CPU cycles, etc.  Not only is this architecture inefficient in its use of network and computational resources: the economics of the arrangement discourage sharing outputs.

## Reimaging distributed open data

If it were possible to short-circuit some of the steps in the architetural pattern above, it could be made more efficient.  For example, what if the decoding steps could be removed?  For that matter, what if the databases could be removed, and the network itself could be the database?

```
  Network -> Process -> Network
```



<!--
Schemes for linking distributed data sets also exist, particularly in the form of Linked Data [@heath2011linked], but the methods of querying and accessing data currently involve database access and retrieval
-->




# Resources, queries, hashes and graphs

## Content addressing

## Resource definitions

## Query definitions

## Query execution

# Distributed queries and linked data

# Discussion


<!-- Refs are added automatically by Pandoc after this next section -->
# References
